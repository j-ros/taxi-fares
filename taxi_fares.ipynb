{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd\nimport datetime as dt\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport os\n\nprint(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "**Load dataset**\n\nFirst we will load the train.csv dataset. Since this dataset has 55M rows, we will only use the first 1M to build our model to prevent memory issues and speed up preprocessing and model building."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41e7502eb90efe0bf213974dd45d8bf686b5da7c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df =  pd.read_csv('../input/train.csv', nrows = 1_000_000)\ntrain_df.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "67a50c3072827032399c98571e1563975e9fab08"
      },
      "cell_type": "markdown",
      "source": "**Data exploration**\n\nNow we will explore the loaded data to identify outliers and other problems that might need fixing such as null values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13dc61c19a6fd7ec53fb0c4c1947f9e973c745c9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Identify null values\nprint(train_df.isnull().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ed9ec358e20228ebef5f3c79b080f862a247c35"
      },
      "cell_type": "markdown",
      "source": "We have a few rows with null values so it is safe to remove them."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "63af8f6b05047dbca96363a13068b082f17bf33d"
      },
      "cell_type": "code",
      "source": "#Drop rows with null values\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1cdc7b60c4ffa97154278cb7d912ada941d4b004"
      },
      "cell_type": "markdown",
      "source": "Now let's explore the variables in the dataset. First we will look at the first rows to get an idea of the format of the values and then we will plot them to get a sense of their distribution and identify outliers."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0a5761b710bf2b1465c732f5869ad521f84dd1f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Look at the first rows\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7e63eddd28a5786b1cb9f2fced34e260a2fbde6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Plot variables using only 1000 rows for efficiency\ntrain_df.iloc[:1000].plot.scatter('pickup_longitude', 'pickup_latitude')\ntrain_df.iloc[:1000].plot.scatter('dropoff_longitude', 'dropoff_latitude')\n\n#Get distribution of values\ntrain_df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8df4397ae5fee4edb2c7fdd0e03b40b0ec1c7182"
      },
      "cell_type": "markdown",
      "source": "Okay, that was interesting. We learned a few things about the dataset:\n- Fare_amount has negative values. We will remove those.\n- Latitudes and longitudes have values near 0 that cannot be correct since NYC is at (40,-74) aprox. We will remove points not near these coordinates.\n- Passenger_count has values of 0 and as high as 200, which are also unrealistic. We will remove those.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "286761fca37f938207d1bee8a7b4ede99388fdd1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Clean dataset\ndef clean_df(df):\n    return df[(df.fare_amount > 0) & \n            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &\n            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &\n            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &\n            (df.passenger_count > 0) & (df.passenger_count < 10)]\n\ntrain_df = clean_df(train_df)\nprint(len(train_df))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "759a91c11c07b15d5b7f8c6b1ee61cb3aa3bd61e"
      },
      "cell_type": "markdown",
      "source": "**Feature engineering**\n\nNow that we have cleaned some extreme values, we will add some interesting features in the dataset.\n- total_distance: distance from pickup to dropoff\n- Extract information from datetime (day of week, month, hour, day)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf4dc24dc12477bb24c32ee52f722fe12902ea51",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n    \n    return 2 * R_earth * np.arcsin(np.sqrt(a))\n\ndef add_datetime_info(dataset):\n    #Convert to datetime format\n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'])\n    \n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n    \n    return dataset\n\ntrain_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], \n                                   train_df['dropoff_latitude'] , train_df['dropoff_longitude'])\n\ntrain_df = add_datetime_info(train_df)\n\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "158fd6b7a6d9a584418ec1a0be2b0cbe9311e629",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Now we need to drop the columns that we will not use to train our model.\n- key\n- pickup_datetime"
    },
    {
      "metadata": {
        "_uuid": "2a7ceec12850b9eaddc6e6f5500d8916459cc6cb",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f3775b9a6c2de1f613672b5b65ed47017462cf0b"
      },
      "cell_type": "markdown",
      "source": "**Model training**\n\nNow that we have the dataframe that we wanted we can start to train the XGBoost model. First we will split the dataset into train (80%)  and test (20%). "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2e97a94627615364085834bc9bc282be060599d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "y = train_df['fare_amount']\ntrain = train_df.drop(columns=['fare_amount'])\n\nx_train,x_test,y_train,y_test = train_test_split(train,y,random_state=0,test_size=0.2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7cfa5cd524f30e6fbcc4c4c788b2a8c8703221de",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'},\n                    dtrain=matrix_train,num_boost_round=100, \n                    early_stopping_rounds=10,evals=[(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(x_train,x_test,y_train,y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ecdfbfa119de80585d53df3a1dddc3771b438a72"
      },
      "cell_type": "markdown",
      "source": "**Prediction**\n\nFinally we can use our trained model to predict the submission. First we will need to load and preprocess the test dataset just like we did for the training dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c326448d0f49e7a1547da0820ac931c20a5dabb"
      },
      "cell_type": "code",
      "source": "#Read and preprocess test set\ntest_df =  pd.read_csv('../input/test.csv')\ntest_df['distance'] = sphere_dist(test_df['pickup_latitude'], test_df['pickup_longitude'], \n                                   test_df['dropoff_latitude'] , test_df['dropoff_longitude'])\ntest_df = add_datetime_info(test_df)\ntest_key = test_df['key']\nx_pred = test_df.drop(columns=['key', 'pickup_datetime'])\n\n#Predict from test set\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e0bd0c431cb912aa91092c9f6fdb5e2cbf40669"
      },
      "cell_type": "code",
      "source": "#Create submission file\nsubmission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "                                key  fare_amount\n0       2015-01-27 13:08:24.0000002     9.490000\n1       2015-01-27 13:08:24.0000003     9.410000\n2       2011-10-08 11:53:44.0000002     5.160000\n3       2012-12-01 21:12:12.0000002     8.850000\n4       2012-12-01 21:12:12.0000003    14.390000\n5       2012-12-01 21:12:12.0000005     9.950000\n6       2011-10-06 12:10:20.0000001     5.590000\n7       2011-10-06 12:10:20.0000003    53.689999\n8       2011-10-06 12:10:20.0000002    12.510000\n9       2014-02-18 15:22:20.0000002     6.250000\n10      2014-02-18 15:22:20.0000003     9.820000\n11      2014-02-18 15:22:20.0000001    15.120000\n12      2010-03-29 20:20:32.0000002     4.390000\n13      2010-03-29 20:20:32.0000001     6.950000\n14      2011-10-06 03:59:12.0000002     7.960000\n15      2011-10-06 03:59:12.0000001    12.730000\n16      2012-07-15 16:45:04.0000006     4.820000\n17      2012-07-15 16:45:04.0000002     8.930000\n18      2012-07-15 16:45:04.0000003     5.020000\n19      2012-07-15 16:45:04.0000004     4.870000\n20      2014-10-29 02:09:56.0000001     6.670000\n21     2014-06-14 13:39:00.00000010     7.630000\n22     2014-06-14 13:39:00.00000060     6.780000\n23     2014-06-14 13:39:00.00000087     8.000000\n24     2014-06-14 13:39:00.00000050    16.660000\n25      2014-06-14 13:39:00.0000003     6.560000\n26    2014-06-14 13:39:00.000000158    35.480000\n27     2014-06-14 13:39:00.00000015    22.799999\n28     2014-06-14 13:39:00.00000073     5.800000\n29     2014-06-14 13:39:00.00000077    14.590000\n...                             ...          ...\n9884   2013-09-25 22:00:00.00000060    38.259998\n9885  2013-09-25 22:00:00.000000213    15.420000\n9886  2013-09-25 22:00:00.000000150    18.670000\n9887   2013-09-25 22:00:00.00000010     7.100000\n9888  2013-09-25 22:00:00.000000146    10.630000\n9889   2013-09-25 22:00:00.00000041     9.510000\n9890  2013-09-25 22:00:00.000000109    10.040000\n9891  2013-09-25 22:00:00.000000210    15.510000\n9892  2013-09-25 22:00:00.000000151     8.970000\n9893  2013-09-25 22:00:00.000000190    14.120000\n9894  2013-09-25 22:00:00.000000153    10.760000\n9895  2013-09-25 22:00:00.000000241    26.570000\n9896  2013-09-25 22:00:00.000000127     9.730000\n9897    2015-02-20 11:08:29.0000001    15.670000\n9898    2015-01-12 15:36:37.0000002     4.770000\n9899    2015-06-07 00:38:14.0000002    16.040001\n9900    2015-04-12 21:56:22.0000005     7.500000\n9901    2015-04-10 11:56:54.0000004     7.970000\n9902    2015-06-25 01:01:46.0000002    13.240000\n9903    2015-05-29 10:02:42.0000001    10.920000\n9904    2015-06-30 20:03:50.0000002    39.400002\n9905    2015-02-27 19:36:02.0000006    22.730000\n9906    2015-06-15 01:00:06.0000002     4.720000\n9907    2015-02-03 09:00:58.0000001    40.080002\n9908    2015-05-19 13:58:11.0000001     8.240000\n9909    2015-05-10 12:37:51.0000002     8.370000\n9910    2015-01-12 17:05:51.0000001    10.690000\n9911    2015-04-19 20:44:15.0000001    51.830002\n9912    2015-01-31 01:05:19.0000005    19.809999\n9913    2015-01-18 14:06:23.0000006     6.400000\n\n[9914 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>fare_amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-27 13:08:24.0000002</td>\n      <td>9.490000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-01-27 13:08:24.0000003</td>\n      <td>9.410000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-10-08 11:53:44.0000002</td>\n      <td>5.160000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-12-01 21:12:12.0000002</td>\n      <td>8.850000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-12-01 21:12:12.0000003</td>\n      <td>14.390000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2012-12-01 21:12:12.0000005</td>\n      <td>9.950000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2011-10-06 12:10:20.0000001</td>\n      <td>5.590000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2011-10-06 12:10:20.0000003</td>\n      <td>53.689999</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2011-10-06 12:10:20.0000002</td>\n      <td>12.510000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2014-02-18 15:22:20.0000002</td>\n      <td>6.250000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2014-02-18 15:22:20.0000003</td>\n      <td>9.820000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2014-02-18 15:22:20.0000001</td>\n      <td>15.120000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2010-03-29 20:20:32.0000002</td>\n      <td>4.390000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2010-03-29 20:20:32.0000001</td>\n      <td>6.950000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2011-10-06 03:59:12.0000002</td>\n      <td>7.960000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2011-10-06 03:59:12.0000001</td>\n      <td>12.730000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2012-07-15 16:45:04.0000006</td>\n      <td>4.820000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2012-07-15 16:45:04.0000002</td>\n      <td>8.930000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2012-07-15 16:45:04.0000003</td>\n      <td>5.020000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2012-07-15 16:45:04.0000004</td>\n      <td>4.870000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2014-10-29 02:09:56.0000001</td>\n      <td>6.670000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2014-06-14 13:39:00.00000010</td>\n      <td>7.630000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2014-06-14 13:39:00.00000060</td>\n      <td>6.780000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2014-06-14 13:39:00.00000087</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2014-06-14 13:39:00.00000050</td>\n      <td>16.660000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2014-06-14 13:39:00.0000003</td>\n      <td>6.560000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2014-06-14 13:39:00.000000158</td>\n      <td>35.480000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2014-06-14 13:39:00.00000015</td>\n      <td>22.799999</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2014-06-14 13:39:00.00000073</td>\n      <td>5.800000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>2014-06-14 13:39:00.00000077</td>\n      <td>14.590000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9884</th>\n      <td>2013-09-25 22:00:00.00000060</td>\n      <td>38.259998</td>\n    </tr>\n    <tr>\n      <th>9885</th>\n      <td>2013-09-25 22:00:00.000000213</td>\n      <td>15.420000</td>\n    </tr>\n    <tr>\n      <th>9886</th>\n      <td>2013-09-25 22:00:00.000000150</td>\n      <td>18.670000</td>\n    </tr>\n    <tr>\n      <th>9887</th>\n      <td>2013-09-25 22:00:00.00000010</td>\n      <td>7.100000</td>\n    </tr>\n    <tr>\n      <th>9888</th>\n      <td>2013-09-25 22:00:00.000000146</td>\n      <td>10.630000</td>\n    </tr>\n    <tr>\n      <th>9889</th>\n      <td>2013-09-25 22:00:00.00000041</td>\n      <td>9.510000</td>\n    </tr>\n    <tr>\n      <th>9890</th>\n      <td>2013-09-25 22:00:00.000000109</td>\n      <td>10.040000</td>\n    </tr>\n    <tr>\n      <th>9891</th>\n      <td>2013-09-25 22:00:00.000000210</td>\n      <td>15.510000</td>\n    </tr>\n    <tr>\n      <th>9892</th>\n      <td>2013-09-25 22:00:00.000000151</td>\n      <td>8.970000</td>\n    </tr>\n    <tr>\n      <th>9893</th>\n      <td>2013-09-25 22:00:00.000000190</td>\n      <td>14.120000</td>\n    </tr>\n    <tr>\n      <th>9894</th>\n      <td>2013-09-25 22:00:00.000000153</td>\n      <td>10.760000</td>\n    </tr>\n    <tr>\n      <th>9895</th>\n      <td>2013-09-25 22:00:00.000000241</td>\n      <td>26.570000</td>\n    </tr>\n    <tr>\n      <th>9896</th>\n      <td>2013-09-25 22:00:00.000000127</td>\n      <td>9.730000</td>\n    </tr>\n    <tr>\n      <th>9897</th>\n      <td>2015-02-20 11:08:29.0000001</td>\n      <td>15.670000</td>\n    </tr>\n    <tr>\n      <th>9898</th>\n      <td>2015-01-12 15:36:37.0000002</td>\n      <td>4.770000</td>\n    </tr>\n    <tr>\n      <th>9899</th>\n      <td>2015-06-07 00:38:14.0000002</td>\n      <td>16.040001</td>\n    </tr>\n    <tr>\n      <th>9900</th>\n      <td>2015-04-12 21:56:22.0000005</td>\n      <td>7.500000</td>\n    </tr>\n    <tr>\n      <th>9901</th>\n      <td>2015-04-10 11:56:54.0000004</td>\n      <td>7.970000</td>\n    </tr>\n    <tr>\n      <th>9902</th>\n      <td>2015-06-25 01:01:46.0000002</td>\n      <td>13.240000</td>\n    </tr>\n    <tr>\n      <th>9903</th>\n      <td>2015-05-29 10:02:42.0000001</td>\n      <td>10.920000</td>\n    </tr>\n    <tr>\n      <th>9904</th>\n      <td>2015-06-30 20:03:50.0000002</td>\n      <td>39.400002</td>\n    </tr>\n    <tr>\n      <th>9905</th>\n      <td>2015-02-27 19:36:02.0000006</td>\n      <td>22.730000</td>\n    </tr>\n    <tr>\n      <th>9906</th>\n      <td>2015-06-15 01:00:06.0000002</td>\n      <td>4.720000</td>\n    </tr>\n    <tr>\n      <th>9907</th>\n      <td>2015-02-03 09:00:58.0000001</td>\n      <td>40.080002</td>\n    </tr>\n    <tr>\n      <th>9908</th>\n      <td>2015-05-19 13:58:11.0000001</td>\n      <td>8.240000</td>\n    </tr>\n    <tr>\n      <th>9909</th>\n      <td>2015-05-10 12:37:51.0000002</td>\n      <td>8.370000</td>\n    </tr>\n    <tr>\n      <th>9910</th>\n      <td>2015-01-12 17:05:51.0000001</td>\n      <td>10.690000</td>\n    </tr>\n    <tr>\n      <th>9911</th>\n      <td>2015-04-19 20:44:15.0000001</td>\n      <td>51.830002</td>\n    </tr>\n    <tr>\n      <th>9912</th>\n      <td>2015-01-31 01:05:19.0000005</td>\n      <td>19.809999</td>\n    </tr>\n    <tr>\n      <th>9913</th>\n      <td>2015-01-18 14:06:23.0000006</td>\n      <td>6.400000</td>\n    </tr>\n  </tbody>\n</table>\n<p>9914 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "575db49166dea9d6fd01f16ebb01ec411d30f6f4"
      },
      "cell_type": "markdown",
      "source": "**Possible improvements**\n\n- Right now, converting the 'pickup_datetime' column to datetime format is a real bottleneck. Try to find a way to better scale this part to be able to train with a larger number of traning samples.\n- Use cross-validation to tune the hyperparameters of the model for better performance."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}